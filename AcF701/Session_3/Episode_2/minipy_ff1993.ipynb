{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fama-French Risk Factors SML & HML\n",
    "by Dr Liang Jin\n",
    "\n",
    "Part of AcF701 Python Sessions: [github.com/drliangjin/mini-python-book](https://github.com/drliangjin/mini-python-book)\n",
    "\n",
    "Based on the Python example on WRDS by Qingyi Song Drechsler: [Fama-French Factors (Python)](https://wrds-www.wharton.upenn.edu/pages/support/applications/risk-factors-and-industry-benchmarks/fama-french-factors-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Import external packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# import pacakges\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import wrds\n",
    "import psycopg2 \n",
    "import matplotlib.pyplot as plt\n",
    "from dateutil.relativedelta import *\n",
    "from pandas.tseries.offsets import *\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Connect to WRDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# WRDS Connection\n",
    "conn = wrds.Connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Retrieving Compustat Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Postgres Query\n",
    "stmt = \"\"\"\n",
    "          SELECT gvkey, datadate, at, pstkl, txditc, pstkrv, seq, pstk\n",
    "          FROM comp.funda\n",
    "          WHERE indfmt='INDL' \n",
    "          AND datafmt='STD'\n",
    "          AND popsrc='D'\n",
    "          AND consol='C'\n",
    "          AND datadate >= '01/01/1959'\n",
    "       \"\"\"\n",
    "comp = conn.raw_sql(stmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# basic info on the data\n",
    "comp.info()\n",
    "\n",
    "# a closer look at the data\n",
    "comp.describe()\n",
    "\n",
    "# head and tail\n",
    "comp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Work on Compustat Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# set date and time to the standard format recognised by Pandas and other packages\n",
    "comp['datadate']=pd.to_datetime(comp['datadate'])\n",
    "\n",
    "# create a new variable for year\n",
    "comp['year']=comp['datadate'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# check data\n",
    "comp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Deal with prefered stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# if pstkrv is missing, then use pstkl\n",
    "comp['ps']=np.where(comp['pstkrv'].isnull(), comp['pstkl'], comp['pstkrv'])\n",
    "\n",
    "# if created ps is missing, then use pstk\n",
    "comp['ps']=np.where(comp['ps'].isnull(),comp['pstk'], comp['ps'])\n",
    "\n",
    "# if ps is still missing, then assign 0\n",
    "comp['ps']=np.where(comp['ps'].isnull(),0,comp['ps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# again check prefered stock we just created\n",
    "comp['ps'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Book Value of Equity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# assign 0 to txditc\n",
    "comp['txditc']=comp['txditc'].fillna(0)\n",
    "\n",
    "# create a variable, be, for book value of equity\n",
    "comp['be']=comp['seq']+comp['txditc']-comp['ps']\n",
    "\n",
    "# if be is missing, replaced by NaN \n",
    "comp['be']=np.where(comp['be']>0, comp['be'], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# check book value of equity\n",
    "comp['be'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### House cleanning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# sort values so that the dataframe is constructed by id and time\n",
    "comp=comp.sort_values(by=['gvkey','datadate'])\n",
    "\n",
    "# count obs? starting from 0 to length of the group -1\n",
    "comp['count']=comp.groupby(['gvkey']).cumcount()\n",
    "\n",
    "# house cleanning\n",
    "comp=comp[['gvkey','datadate','year','be','count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "comp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Retrieving CRSP stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# sql for returning a merged crsp price dataset\n",
    "stmt = \"\"\"\n",
    "          SELECT a.permno, a.permco, a.date, a.ret, a.retx, a.shrout, a.prc,\n",
    "                 b.shrcd, b.exchcd\n",
    "          FROM crsp.msf AS a\n",
    "          LEFT JOIN crsp.msenames AS b\n",
    "          ON a.permno=b.permno\n",
    "          AND b.namedt<=a.date\n",
    "          AND a.date<=b.nameendt\n",
    "          WHERE a.date BETWEEN '01/01/1959' AND '12/31/2017'\n",
    "          AND b.exchcd BETWEEN 1 AND 3\n",
    "       \"\"\"\n",
    "crsp_m = conn.raw_sql(stmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# check data\n",
    "crsp_m.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data cleanning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# change variable format to int\n",
    "crsp_m[['permco','permno','shrcd','exchcd']]=crsp_m[['permco','permno','shrcd','exchcd']].astype(int)\n",
    "\n",
    "# personally, I like to set all the identifications to be strings...because of the annoying trailing zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# format datatime\n",
    "crsp_m['date']=pd.to_datetime(crsp_m['date'])\n",
    "\n",
    "# MonthEnd is a function from pandas.tseries.offsets\n",
    "# convert timestamp to current month end <= for easier merging purpose\n",
    "# MonthEnd(-1) move backwards by 1 month (last month end)\n",
    "# MonthEnd(1) next month end\n",
    "crsp_m['jdate']=crsp_m['date']+MonthEnd(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Add delisting return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# again, sql query\n",
    "dlret = conn.raw_sql(\"SELECT permno, dlret, dlstdt FROM crsp.msedelist\")\n",
    "\n",
    "# work on datetime\n",
    "dlret['dlstdt']=pd.to_datetime(dlret['dlstdt'])\n",
    "dlret['jdate']=dlret['dlstdt']+MonthEnd(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# merge two datasets\n",
    "crsp = pd.merge(crsp_m, dlret, how='left',on=['permno','jdate'])\n",
    "\n",
    "# house cleaning\n",
    "crsp['dlret']=crsp['dlret'].fillna(0)\n",
    "crsp['ret']=crsp['ret'].fillna(0)\n",
    "crsp['retadj']=(1+crsp['ret'])*(1+crsp['dlret'])-1\n",
    "crsp['me']=crsp['prc'].abs()*crsp['shrout'] # calculate market equity\n",
    "crsp=crsp.drop(['dlret','dlstdt','prc','shrout'], axis=1)\n",
    "crsp=crsp.sort_values(by=['jdate','permco','me'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aggregate market-cap to company level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# sum of me across different permno belonging to same permco a given date\n",
    "crsp_summe = crsp.groupby(['jdate','permco'])['me'].sum().reset_index()\n",
    "# largest mktcap within a permco/date\n",
    "crsp_maxme = crsp.groupby(['jdate','permco'])['me'].max().reset_index()\n",
    "\n",
    "# join by jdate/maxme to find the permno\n",
    "crsp1=pd.merge(crsp, crsp_maxme, how='inner', on=['jdate','permco','me'])\n",
    "# drop me column and replace with the sum me\n",
    "crsp1=crsp1.drop(['me'], axis=1)\n",
    "# join with sum of me to get the correct market cap info\n",
    "crsp2=pd.merge(crsp1, crsp_summe, how='inner', on=['jdate','permco'])\n",
    "# sort by permno and date and also drop duplicates\n",
    "crsp2=crsp2.sort_values(by=['permno','jdate']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Work on FF datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# create year and month\n",
    "crsp2['year']=crsp2['jdate'].dt.year\n",
    "crsp2['month']=crsp2['jdate'].dt.month\n",
    "# keep December market cap\n",
    "decme=crsp2[crsp2['month']==12]\n",
    "decme=decme[['permno','date','jdate','me','year']].rename(columns={'me':'dec_me'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "### July to June dates\n",
    "crsp2['ffdate']=crsp2['jdate']+MonthEnd(-6)\n",
    "crsp2['ffyear']=crsp2['ffdate'].dt.year\n",
    "crsp2['ffmonth']=crsp2['ffdate'].dt.month\n",
    "crsp2['1+retx']=1+crsp2['retx']\n",
    "crsp2=crsp2.sort_values(by=['permno','date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stock level characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# cumret by stock\n",
    "crsp2['cumretx']=crsp2.groupby(['permno','ffyear'])['1+retx'].cumprod()\n",
    "# lag cumret\n",
    "crsp2['lcumretx']=crsp2.groupby(['permno'])['cumretx'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# lag market cap\n",
    "crsp2['lme']=crsp2.groupby(['permno'])['me'].shift(1)\n",
    "\n",
    "# if first permno then use me/(1+retx) to replace the missing value\n",
    "crsp2['count']=crsp2.groupby(['permno']).cumcount()\n",
    "crsp2['lme']=np.where(crsp2['count']==0, crsp2['me']/crsp2['1+retx'], crsp2['lme'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# baseline me\n",
    "mebase=crsp2[crsp2['ffmonth']==1][['permno','ffyear', 'lme']].rename(columns={'lme':'mebase'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### House cleanning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# merge result back together\n",
    "crsp3=pd.merge(crsp2, mebase, how='left', on=['permno','ffyear'])\n",
    "crsp3['wt']=np.where(crsp3['ffmonth']==1, crsp3['lme'], crsp3['mebase']*crsp3['lcumretx'])\n",
    "\n",
    "decme['year']=decme['year']+1\n",
    "decme=decme[['permno','year','dec_me']]\n",
    "\n",
    "# Info as of June\n",
    "crsp3_jun = crsp3[crsp3['month']==6]\n",
    "\n",
    "crsp_jun = pd.merge(crsp3_jun, decme, how='inner', on=['permno','year'])\n",
    "crsp_jun=crsp_jun[['permno','date', 'jdate', 'shrcd','exchcd','retadj','me','wt','cumretx','mebase','lme','dec_me']]\n",
    "crsp_jun=crsp_jun.sort_values(by=['permno','jdate']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Retrieving CCM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ccm=conn.raw_sql(\"\"\"\n",
    "                    SELECT gvkey, lpermno AS permno, linktype, linkprim, \n",
    "                    linkdt, linkenddt\n",
    "                    FROM crsp.ccmxpf_linktable\n",
    "                    WHERE SUBSTR(linktype,1,1)='L'\n",
    "                    AND (linkprim ='C' OR linkprim='P')\n",
    "                 \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# convert datetime\n",
    "ccm['linkdt']=pd.to_datetime(ccm['linkdt'])\n",
    "ccm['linkenddt']=pd.to_datetime(ccm['linkenddt'])\n",
    "# if linkenddt is missing then set to today date\n",
    "ccm['linkenddt']=ccm['linkenddt'].fillna(pd.to_datetime('today'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Merge with Compustat and CRSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# left merge on gvkey\n",
    "ccm1=pd.merge(comp[['gvkey', 'datadate', 'be', 'count']],ccm,how='left',on=['gvkey'])\n",
    "ccm1['yearend']=ccm1['datadate']+YearEnd(0)\n",
    "# create 'jdate' for further merge with crsp dataset\n",
    "ccm1['jdate']=ccm1['yearend']+MonthEnd(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# set link date bounds\n",
    "ccm2=ccm1[(ccm1['jdate']>=ccm1['linkdt']) & (ccm1['jdate']<=ccm1['linkenddt'])]\n",
    "ccm2=ccm2[['gvkey', 'permno', 'datadate', 'yearend', 'jdate', 'be', 'count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# link comp and crsp\n",
    "ccm_jun=pd.merge(crsp_jun, ccm2, how='inner', on=['permno', 'jdate'])\n",
    "\n",
    "# calculate book to market ratio\n",
    "ccm_jun['beme']=ccm_jun['be']*1000/ccm_jun['dec_me']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NYSE stock bucket breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# select NYSE stocks for bucket breakdown\n",
    "# exchcd = 1 and positive beme and positive me and shrcd in (10,11) and at least 2 years in comp\n",
    "nyse=ccm_jun[(ccm_jun['exchcd']==1) & \n",
    "             (ccm_jun['beme']>0) & \n",
    "             (ccm_jun['me']>0) & \n",
    "             (ccm_jun['count']>=1) & \n",
    "             ((ccm_jun['shrcd']==10) | (ccm_jun['shrcd']==11))]\n",
    "\n",
    "# size breakdown\n",
    "nyse_sz=nyse.groupby(['jdate'])['me'].median().to_frame().reset_index().rename(columns={'me':'sizemedn'})\n",
    "\n",
    "# beme breakdown\n",
    "nyse_bm=nyse.groupby(['jdate'])['beme'].describe(percentiles=[0.3, 0.7]).reset_index()\n",
    "nyse_bm=nyse_bm[['jdate','30%','70%']].rename(columns={'30%':'bm30', '70%':'bm70'})\n",
    "\n",
    "# merge two bucket breakdown datasets\n",
    "nyse_breaks = pd.merge(nyse_sz, nyse_bm, how='inner', on=['jdate'])\n",
    "\n",
    "# merge back to our main dataset\n",
    "ccm1_jun = pd.merge(ccm_jun, nyse_breaks, how='left', on=['jdate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Functions for assigning characteristics bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# functions\n",
    "def sz_bucket(row):\n",
    "    if row['me']==np.nan:\n",
    "        value=''\n",
    "    elif row['me']<=row['sizemedn']:\n",
    "        value='S'\n",
    "    else:\n",
    "        value='B'\n",
    "    return value\n",
    "\n",
    "def bm_bucket(row):\n",
    "    if 0<=row['beme']<=row['bm30']:\n",
    "        value = 'L'\n",
    "    elif row['beme']<=row['bm70']:\n",
    "        value='M'\n",
    "    elif row['beme']>row['bm70']:\n",
    "        value='H'\n",
    "    else:\n",
    "        value=''\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Assign size and btm portfolios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# assign size portfolio\n",
    "ccm1_jun['szport']=np.where((ccm1_jun['beme']>0) & \n",
    "                            (ccm1_jun['me']>0) & \n",
    "                            (ccm1_jun['count']>=1), \n",
    "                            ccm1_jun.apply(sz_bucket, axis=1), '')\n",
    "\n",
    "# assign book-to-market portfolio\n",
    "ccm1_jun['bmport']=np.where((ccm1_jun['beme']>0) & \n",
    "                            (ccm1_jun['me']>0) & \n",
    "                            (ccm1_jun['count']>=1), \n",
    "                            ccm1_jun.apply(bm_bucket, axis=1), '')\n",
    "\n",
    "# create positivebmeme and nonmissport variable\n",
    "ccm1_jun['posbm']=np.where((ccm1_jun['beme']>0) & \n",
    "                           (ccm1_jun['me']>0) & \n",
    "                           (ccm1_jun['count']>=1), \n",
    "                           1, 0)\n",
    "ccm1_jun['nonmissport']=np.where((ccm1_jun['bmport']!=''), 1, 0)\n",
    "\n",
    "# store portfolio assignment as of June\n",
    "june=ccm1_jun[['permno','date', 'jdate', 'bmport','szport','posbm','nonmissport']]\n",
    "june['ffyear']=june['jdate'].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Merge with monthly returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# merge back with monthly records\n",
    "crsp3 = crsp3[['date','permno','shrcd','exchcd','retadj',\n",
    "               'me','wt','cumretx','ffyear','jdate']]\n",
    "ccm3=pd.merge(crsp3, \n",
    "              june[['permno','ffyear','szport','bmport','posbm','nonmissport']], \n",
    "              how='left', \n",
    "              on=['permno','ffyear'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# keeping only records that meet the criteria\n",
    "ccm4=ccm3[(ccm3['wt']>0) & \n",
    "          (ccm3['posbm']==1) &\n",
    "          (ccm3['nonmissport']==1) & \n",
    "          ((ccm3['shrcd']==10) | (ccm3['shrcd']==11))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Value-weighted returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# function to calculate value weighted return\n",
    "def wavg(group, avg_name, weight_name):\n",
    "    d = group[avg_name]\n",
    "    w = group[weight_name]\n",
    "    try:\n",
    "        return (d * w).sum() / w.sum()\n",
    "    except ZeroDivisionError:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# value-weigthed return\n",
    "vwret=ccm4.groupby(['jdate','szport','bmport']).apply(wavg, 'retadj', 'wt').to_frame().reset_index().rename(columns={0: 'vwret'})\n",
    "vwret['sbport']=vwret['szport']+vwret['bmport'] # <= concat string\n",
    "\n",
    "# firm count\n",
    "vwret_n=ccm4.groupby(['jdate','szport','bmport'])['retadj'].count().reset_index().rename(columns={'retadj':'n_firms'})\n",
    "vwret_n['sbport']=vwret_n['szport']+vwret_n['bmport']\n",
    "\n",
    "ff_factors=vwret.pivot(index='jdate', columns='sbport', values='vwret').reset_index()\n",
    "ff_nfirms=vwret_n.pivot(index='jdate', columns='sbport', values='n_firms').reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Create SMB and HML factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# create SMB and HML factors\n",
    "ff_factors['WH']=(ff_factors['BH']+ff_factors['SH'])/2\n",
    "ff_factors['WL']=(ff_factors['BL']+ff_factors['SL'])/2\n",
    "ff_factors['WHML'] = ff_factors['WH']-ff_factors['WL']\n",
    "\n",
    "ff_factors['WB']=(ff_factors['BL']+ff_factors['BM']+ff_factors['BH'])/3\n",
    "ff_factors['WS']=(ff_factors['SL']+ff_factors['SM']+ff_factors['SH'])/3\n",
    "ff_factors['WSMB'] = ff_factors['WS']-ff_factors['WB']\n",
    "ff_factors=ff_factors.rename(columns={'jdate':'date'})\n",
    "\n",
    "# n firm count\n",
    "ff_nfirms['H']=ff_nfirms['SH']+ff_nfirms['BH']\n",
    "ff_nfirms['L']=ff_nfirms['SL']+ff_nfirms['BL']\n",
    "ff_nfirms['HML']=ff_nfirms['H']+ff_nfirms['L']\n",
    "\n",
    "ff_nfirms['B']=ff_nfirms['BL']+ff_nfirms['BM']+ff_nfirms['BH']\n",
    "ff_nfirms['S']=ff_nfirms['SL']+ff_nfirms['SM']+ff_nfirms['SH']\n",
    "ff_nfirms['SMB']=ff_nfirms['B']+ff_nfirms['S']\n",
    "ff_nfirms['TOTAL']=ff_nfirms['SMB']\n",
    "ff_nfirms=ff_nfirms.rename(columns={'jdate':'date'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Compare with FF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# download data from wrds\n",
    "_ff = conn.get_table(library='ff', table='factors_monthly')\n",
    "_ff=_ff[['date','smb','hml']]\n",
    "_ff['date']=_ff['date']+MonthEnd(0)\n",
    "\n",
    "# correlation between our created FF factors and published FF\n",
    "_ffcomp = pd.merge(_ff, ff_factors[['date','WSMB','WHML']], how='inner', on=['date'])\n",
    "_ffcomp70=_ffcomp[_ffcomp['date']>='01/01/1970']\n",
    "print(stats.pearsonr(_ffcomp70['smb'], _ffcomp70['WSMB']))\n",
    "print(stats.pearsonr(_ffcomp70['hml'], _ffcomp70['WHML']))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
